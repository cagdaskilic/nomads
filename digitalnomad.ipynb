{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Trying to see if this makes a change"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPZHDHu9DJip",
        "outputId": "72798215-42c8-4965-c511-b30b7142e21d"
      },
      "outputs": [],
      "source": [
        "# Mounting Google drive to colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls /content/drive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "Everything up-to-date\n"
          ]
        }
      ],
      "source": [
        "!git commit -a -m \"Changes are committed to GitHub\" \n",
        "!git push"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "u-yl4gDRhM1P"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'digitalnomad_comments.json'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(line)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Create the DataFrame using the generator\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Display the DataFrame\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/frame.py:843\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    841\u001b[0m         data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(data)\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 843\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_dataclass(data[\u001b[38;5;241m0\u001b[39m]):\n",
            "Cell \u001b[0;32mIn[2], line 9\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_json\u001b[39m(filename):\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(line)\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'digitalnomad_comments.json'"
          ]
        }
      ],
      "source": [
        "# Reading the JSON file and converting it into pandas DataFrame (df)\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "input_file = 'digitalnomad_comments.json'\n",
        "\n",
        "def read_json(filename):\n",
        "    with open(filename, 'r') as f:\n",
        "        for line in f:\n",
        "            yield json.loads(line)\n",
        "\n",
        "# Create the DataFrame using the generator\n",
        "df = pd.DataFrame(read_json(input_file))\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n",
        "\n",
        "print('DataFrame Properties:')\n",
        "print(type(df))\n",
        "\n",
        " # Count the keys (which will become column names in a DataFrame)\n",
        "num_keys = len(df.keys())\n",
        "print(\"Number of keys (future column names):\", num_keys)\n",
        "\n",
        "# Print the keys (future column names)\n",
        "print(\"Keys (future column names):\", list(df.keys()))\n",
        "\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYnYIpd9rFnm",
        "outputId": "de4ce341-f500-4978-b583-b98f8c3a261f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame size before dropping columns: 261266872 bytes\n",
            "DataFrame size after dropping columns: 261266872 bytes\n",
            "Memory usage reduced by 0 bytes\n",
            "DataFrame Properties:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Number of keys (future column names): 1\n",
            "Keys (future column names): ['text']\n",
            "(745868, 1)\n",
            "                                                     text\n",
            "0                                                      Hi\n",
            "1                                                     Hi!\n",
            "2       What business are you in? And where are you now? \n",
            "3       Tech support for Hostgator in Houston Texas. I...\n",
            "4       Cool! So what's your plan to make money while ...\n",
            "...                                                   ...\n",
            "745863  I'd hazard that many if not the majority of sa...\n",
            "745864                           India, Thailand, Morocco\n",
            "745865  Programming will be one of the last jobs to be...\n",
            "745866              BA easily. Summer, way more fun, etc.\n",
            "745867  6 weeks is the sweet spot for me. \\n\\nIt reall...\n",
            "\n",
            "[745868 rows x 1 columns]\n"
          ]
        }
      ],
      "source": [
        "# Dropping the unnecessary columns to reduce memory/computing usage\n",
        "\n",
        "import sys\n",
        "\n",
        "# Before dropping columns\n",
        "size_before = df.memory_usage(index=True, deep=True).sum()\n",
        "print(f\"DataFrame size before dropping columns: {size_before} bytes\")\n",
        "\n",
        "# Drop all columns except 'body'\n",
        "df = df[['body']]\n",
        "\n",
        "# After dropping columns\n",
        "size_after = df.memory_usage(index=True, deep=True).sum()\n",
        "print(f\"DataFrame size after dropping columns: {size_after} bytes\")\n",
        "\n",
        "# Calculate reduction\n",
        "reduction = size_before - size_after\n",
        "print(f\"Memory usage reduced by {reduction} bytes\")\n",
        "\n",
        "# Let's change the name of our column from 'body' to 'text'\n",
        "df.rename(columns={'body': 'text'}, inplace=True)\n",
        "\n",
        "# Print the updated df and its properties\n",
        "print('DataFrame Properties:')\n",
        "print(type(df))\n",
        "\n",
        "num_keys = len(df.keys())\n",
        "print(\"Number of keys (future column names):\", num_keys)\n",
        "\n",
        "print(\"Keys (future column names):\", list(df.keys()))\n",
        "print(df.shape)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3D7v9hnqeAA",
        "outputId": "811c300e-70bd-4323-88d4-17eb479fd60a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# Preprocessing the data before BERT embeddings\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, pipeline\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import silhouette_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Text cleaning function\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = re.sub(r'\\d+', '', text)      # Remove digits\n",
        "    text = re.sub(r'\\n', ' ', text)      # Remove new lines\n",
        "    return text\n",
        "\n",
        "# Apply text cleaning\n",
        "df['clean_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "# Stop word removal (using default NLTK stop words + custom words)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Add your custom stop words\n",
        "custom_stop_words = {'yeah', 'yep', 'yes','mmhmm', 'agree', 'ive', 'interviewee', 'laugh'}\n",
        "stop_words.update(custom_stop_words)\n",
        "\n",
        "def remove_stop_words(text):\n",
        "    return ' '.join(word for word in text.split() if word not in stop_words)\n",
        "\n",
        "# Now apply the function\n",
        "df['clean_text'] = df['clean_text'].apply(remove_stop_words)\n",
        "\n",
        "# Lemmatization function\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_text(text):\n",
        "    return ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n",
        "\n",
        "# Apply lemmatization\n",
        "df['clean_text'] = df['clean_text'].apply(lemmatize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "AytMgGZstQRm",
        "outputId": "d8c93d45-0341-4973-e28c-cb84807d9565"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, pipeline\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import silhouette_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# BERT embedding extraction\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def get_bert_embeddings(texts):\n",
        "    embeddings = []\n",
        "    for text in texts:\n",
        "        # Explicitly set max_length and truncation\n",
        "        inputs = tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "document_vectors = get_bert_embeddings(df['clean_text'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhCh17c00pG9",
        "outputId": "9ea7adfc-3e3a-4acd-e793-f91940f1ccbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done with the long task\n"
          ]
        }
      ],
      "source": [
        "print('done with the long task')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QI7xSjatjZ9"
      },
      "outputs": [],
      "source": [
        "# Clustering\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Consider integrating PCA here for dimension reduction (not a must, can be beneficial)\n",
        "\n",
        "num_clusters = 5\n",
        "kmeans = KMeans(n_clusters=num_clusters, n_init=10, random_state=42)  # Added n_init\n",
        "df['cluster'] = kmeans.fit_predict(document_vectors)\n",
        "\n",
        "# Evaluate clustering\n",
        "silhouette_avg = silhouette_score(document_vectors, df['cluster'])\n",
        "print(f\"Silhouette Score: {silhouette_avg}\")\n",
        "\n",
        "# Sentiment analysis\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "def analyze_sentiment(texts):\n",
        "    results = []\n",
        "    for text in texts:\n",
        "        # Truncate text before tokenization to ensure it fits within BERT's limit\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "        if len(tokens) > 510: # 512 minus [CLS] and [SEP] tokens\n",
        "            truncated_text = tokenizer.convert_tokens_to_string(tokens[:510])\n",
        "        else:\n",
        "            truncated_text = text\n",
        "        results.append(sentiment_pipeline(truncated_text)[0]['label'])\n",
        "    return results\n",
        "\n",
        "df['sentiment'] = analyze_sentiment(df['clean_text'])\n",
        "\n",
        "\n",
        "# Aggregate sentiment by cluster\n",
        "import pandas as pd\n",
        "\n",
        "sentiment_summary = df.groupby('cluster')['sentiment'].value_counts(normalize=True).unstack().fillna(0)\n",
        "print(sentiment_summary)\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# t-SNE visualization of clusters\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "reduced_data = tsne.fit_transform(document_vectors)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=df['cluster'], cmap='viridis', alpha=0.5)\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.title(\"t-SNE Visualization of Clusters\")\n",
        "plt.show()\n",
        "\n",
        "# Sentiment distribution by cluster\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='cluster', hue='sentiment', data=df)\n",
        "plt.title(\"Sentiment Distribution by Cluster\")\n",
        "plt.show()\n",
        "\n",
        "# Examine Top Terms in Each Cluster\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "def get_top_terms_per_cluster(df, n_terms=10):\n",
        "    vectorizer = CountVectorizer(stop_words='english')\n",
        "    X = vectorizer.fit_transform(df['clean_text'])\n",
        "    feature_names = np.array(vectorizer.get_feature_names_out())\n",
        "\n",
        "    top_terms = {}\n",
        "    for cluster in df['cluster'].unique():\n",
        "        cluster_texts = df[df['cluster'] == cluster]['clean_text']\n",
        "        cluster_X = vectorizer.transform(cluster_texts)\n",
        "        sum_words = cluster_X.sum(axis=0)\n",
        "        words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
        "        words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
        "        top_terms[cluster] = words_freq[:n_terms]\n",
        "    return top_terms\n",
        "\n",
        "top_terms = get_top_terms_per_cluster(df)\n",
        "for cluster, terms in top_terms.items():\n",
        "    print(f\"Cluster {cluster}:\")\n",
        "    print(\", \".join([f\"{term[0]} ({term[1]})\" for term in terms]))\n",
        "    print()\n",
        "\n",
        "# Sample Texts from Each Cluster\n",
        "def sample_texts_by_cluster(df, n_samples=5):\n",
        "    samples = {}\n",
        "    for cluster in df['cluster'].unique():\n",
        "        cluster_texts = df[df['cluster'] == cluster]['clean_text']  # Ensure column name is correct\n",
        "        samples[cluster] = cluster_texts.sample(n=n_samples, random_state=42).tolist()\n",
        "    return samples\n",
        "\n",
        "cluster_samples = sample_texts_by_cluster(df)\n",
        "for cluster, texts in cluster_samples.items():\n",
        "    print(f\"Cluster {cluster} samples:\")\n",
        "    for text in texts:\n",
        "        print(f\"- {text}\")\n",
        "    print()\n",
        "\n",
        "# Cluster Profiling\n",
        "def profile_clusters(df):\n",
        "    cluster_profile = df.groupby('cluster').agg({\n",
        "        'clean_text': lambda texts: ' '.join(texts),\n",
        "        'sentiment': lambda sentiments: sentiments.mode()[0]\n",
        "    }).reset_index()\n",
        "    return cluster_profile\n",
        "\n",
        "cluster_profiles = profile_clusters(df)\n",
        "print(cluster_profiles)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
